{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHjkPIsd50L-",
        "outputId": "ed7fe433-f351-4307-bc70-61cefa6449a5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhPSUjLPEl_t",
        "outputId": "30f8d2e4-a12f-4ce5-915d-bb614d4d8ab8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deap\n",
            "  Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deap) (1.23.5)\n",
            "Installing collected packages: deap\n",
            "Successfully installed deap-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kornia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS0_ds48E0js",
        "outputId": "6b7e512c-7f32-4b60-f235-48feb45fa188"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kornia\n",
            "  Downloading kornia-0.7.0-py2.py3-none-any.whl (705 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.7/705.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia) (23.2)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from kornia) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.1->kornia) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.1->kornia) (1.3.0)\n",
            "Installing collected packages: kornia\n",
            "Successfully installed kornia-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "kG_tm9PPF30M",
        "outputId": "606371b2-c2c7-480a-dd8c-ddc62a17307c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wcwidth<0.3.0,>=0.2.12 (from ftfy)\n",
            "  Downloading wcwidth-0.2.12-py2.py3-none-any.whl (34 kB)\n",
            "Installing collected packages: wcwidth, ftfy\n",
            "  Attempting uninstall: wcwidth\n",
            "    Found existing installation: wcwidth 0.2.10\n",
            "    Uninstalling wcwidth-0.2.10:\n",
            "      Successfully uninstalled wcwidth-0.2.10\n",
            "Successfully installed ftfy-6.1.3 wcwidth-0.2.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "wcwidth"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/SRE/eoe-main/src/eoe/main/train_clip_cifar.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPwQiFQcP3Ow",
        "outputId": "8edb7142-d7fb-48b5-db42-69ce65d310cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-01 14:05:01.322266: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-01 14:05:01.322326: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-01 14:05:01.322372: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-01 14:05:02.469406: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Program started with:\n",
            " {'dataset': 'cifar10', 'oe_dataset': None, 'oe_size': inf, 'batch_size': 128, 'epochs': 80, 'learning_rate': 2e-05, 'weight_decay': 0.001, 'milestones': [50, 60, 70, 75], 'objective': 'clip', 'ad_mode': 'one_vs_rest', 'classes': [3], 'devices': [0], 'iterations': 1, 'load': None, 'comment': 'clip_cifar10_one_vs_rest_E80', 'superdir': '.'}\n",
            "Load CLIP model.\n",
            "100%|████████████████████████████████████████| 354M/354M [00:02<00:00, 133MiB/s]\n",
            "Successfully saved code at /content/drive/MyDrive/SRE/eoe-main/data/results/./log_20231201140522_clip_cifar10_one_vs_rest_E80/src.tar.gz\n",
            "------ start training cls 3 \"cat\" ------\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "training cls3 ep: 5/80  loss:2.63e-03  roc:None  lr:2.00e-05:   6% 203/3280 [02:05<24:15,  2.11it/s]Model weights saved at epoch 5 to: /content/drive/MyDrive/SRE/eoe-main/saved_model/model_weights_cls3_seed0_epoch5.pth\n",
            "training cls3 ep:10/80  loss:1.61e-03  roc:None  lr:2.00e-05:  12% 408/3280 [04:09<19:42,  2.43it/s]Model weights saved at epoch 10 to: /content/drive/MyDrive/SRE/eoe-main/saved_model/model_weights_cls3_seed0_epoch10.pth\n",
            "training cls3 ep:15/80  loss:1.01e-03  roc:None  lr:2.00e-05:  19% 613/3280 [06:11<18:08,  2.45it/s]Model weights saved at epoch 15 to: /content/drive/MyDrive/SRE/eoe-main/saved_model/model_weights_cls3_seed0_epoch15.pth\n",
            "training cls3 ep:20/80  loss:3.38e-03  roc:None  lr:2.00e-05:  25% 818/3280 [08:13<16:11,  2.53it/s]Model weights saved at epoch 20 to: /content/drive/MyDrive/SRE/eoe-main/saved_model/model_weights_cls3_seed0_epoch20.pth\n",
            "training cls3 ep:25/80  loss:1.04e-03  roc:None  lr:2.00e-05:  31% 1023/3280 [10:15<15:03,  2.50it/s]Model weights saved at epoch 25 to: /content/drive/MyDrive/SRE/eoe-main/saved_model/model_weights_cls3_seed0_epoch25.pth\n",
            "training cls3 ep:30/80  loss:9.52e-04  roc:None  lr:2.00e-05:  37% 1228/3280 [12:12<14:53,  2.30it/s]Model weights saved at epoch 30 to: /content/drive/MyDrive/SRE/eoe-main/saved_model/model_weights_cls3_seed0_epoch30.pth\n",
            "training cls3 ep:35/80  loss:9.83e-04  roc:None  lr:2.00e-05:  44% 1433/3280 [14:05<11:48,  2.61it/s]Model weights saved at epoch 35 to: /content/drive/MyDrive/SRE/eoe-main/saved_model/model_weights_cls3_seed0_epoch35.pth\n",
            "training cls3 ep:40/80  loss:6.57e-04  roc:None  lr:2.00e-05:  50% 1638/3280 [16:02<11:46,  2.32it/s]Model weights saved at epoch 40 to: /content/drive/MyDrive/SRE/eoe-main/saved_model/model_weights_cls3_seed0_epoch40.pth\n",
            "training cls3 ep:45/80  loss:7.41e-04  roc:None  lr:2.00e-05:  56% 1843/3280 [17:58<15:18,  1.56it/s]Model weights saved at epoch 45 to: /content/drive/MyDrive/SRE/eoe-main/saved_model/model_weights_cls3_seed0_epoch45.pth\n",
            "training cls3 ep:50/80  loss:5.95e-04  roc:None  lr:2.00e-05:  62% 2048/3280 [19:52<08:59,  2.28it/s]Model weights saved at epoch 50 to: /content/drive/MyDrive/SRE/eoe-main/saved_model/model_weights_cls3_seed0_epoch50.pth\n",
            "training cls3 ep:55/80  loss:8.52e-04  roc:None  lr:2.00e-06:  69% 2253/3280 [22:01<09:09,  1.87it/s]Model weights saved at epoch 55 to: /content/drive/MyDrive/SRE/eoe-main/saved_model/model_weights_cls3_seed0_epoch55.pth\n",
            "training cls3 ep:60/80  loss:1.28e-03  roc:None  lr:2.00e-06:  75% 2458/3280 [23:56<05:42,  2.40it/s]Model weights saved at epoch 60 to: /content/drive/MyDrive/SRE/eoe-main/saved_model/model_weights_cls3_seed0_epoch60.pth\n",
            "training cls3 ep:65/80  loss:8.97e-04  roc:None  lr:2.00e-07:  81% 2663/3280 [25:54<04:15,  2.41it/s]Model weights saved at epoch 65 to: /content/drive/MyDrive/SRE/eoe-main/saved_model/model_weights_cls3_seed0_epoch65.pth\n",
            "training cls3 ep:70/80  loss:7.75e-04  roc:None  lr:2.00e-07:  87% 2868/3280 [27:51<04:22,  1.57it/s]Model weights saved at epoch 70 to: /content/drive/MyDrive/SRE/eoe-main/saved_model/model_weights_cls3_seed0_epoch70.pth\n",
            "training cls3 ep:75/80  loss:6.26e-04  roc:None  lr:2.00e-08:  94% 3073/3280 [29:47<01:34,  2.19it/s]Model weights saved at epoch 75 to: /content/drive/MyDrive/SRE/eoe-main/saved_model/model_weights_cls3_seed0_epoch75.pth\n",
            "training cls3 ep:80/80  loss:7.51e-04  roc:None  lr:2.00e-09: 100% 3278/3280 [31:43<00:00,  2.53it/s]Model weights saved at epoch 80 to: /content/drive/MyDrive/SRE/eoe-main/saved_model/model_weights_cls3_seed0_epoch80.pth\n",
            "training cls3 ep:80/80  loss:6.91e-04  roc:None  lr:2.00e-09: 100% 3280/3280 [31:44<00:00,  1.72it/s]\n",
            "evaluating cls cat: 100% 79/79 [00:24<00:00,  3.20it/s]\n",
            "Eval: class \"cat\" yields 95.58% AUC and 99.51% average precision (seed 0).\n",
            "--------------- OVERVIEW ------------------\n",
            "Eval: Overall 99.51% +- 0.00% AvgPrec.\n",
            "Eval: Class \"cat\" yields 95.58% +- 0.00% AUC.\n",
            "Eval: Overall 95.58% +- 0.00% AUC.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import torch\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import eoe.models.clip_official.clip as official_clip\n",
        "\n",
        "# Load the CLIP model class\n",
        "model = official_clip # Make sure to adjust this based on your actual model class\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/SRE/eoe-main/saved_model/model_weights_cls3_seed0_epoch80.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Define test image transformation\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load and preprocess test image\n",
        "image_folder_path = '/content/drive/MyDrive/SRE/test_images'\n",
        "test_images = []\n",
        "for i in range(4):\n",
        "  image_path = image_folder_path + \"/img_\"+i+\".jpeg\"\n",
        "  test_image = Image.open(image_path)\n",
        "  test_image = test_transform(test_image).unsqueeze(0)  # Add batch dimension\n",
        "  test_images.append(test_image)\n",
        "\n",
        "# Run inference\n",
        "for i in range(4):\n",
        "  with torch.no_grad():\n",
        "      output = model(test_images[i])\n",
        "\n",
        "    # Define anomaly detection threshold\n",
        "  threshold = 0.5  # Adjust this threshold based on your use case\n",
        "\n",
        "  # Perform anomaly detection\n",
        "  if output.item() > threshold:\n",
        "      print(\"Anomaly Detected!\")\n",
        "  else:\n",
        "      print(\"Normal Image.\")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "EDEfnbS1axQg",
        "outputId": "3ceef0e4-ed8c-4574-8fd0-a036058f5199"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-cbafb9f7f8f3>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0meoe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_official\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mofficial_clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the CLIP model class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'eoe'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import torch\n",
        "from torchvision.transforms import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Function to load the CLIP model\n",
        "def load_clip_model(model_path):\n",
        "    model = torch.load(model_path)\n",
        "    return model\n",
        "\n",
        "# Function to preprocess input image\n",
        "def preprocess_image(image_path):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x + 0.001 * torch.randn_like(x)),\n",
        "    ])\n",
        "\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image)\n",
        "    image = image.unsqueeze(0)  # Add batch dimension\n",
        "    return image\n",
        "\n",
        "# Function for anomaly detection\n",
        "def detect_anomaly(model, image_tensor):\n",
        "    # Your logic for anomaly detection using the CLIP model\n",
        "    # You can use model(image_tensor) and analyze the output for anomaly detection\n",
        "\n",
        "    # For example, you can print the model's prediction\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor)\n",
        "    print(\"Model Prediction:\", output)\n",
        "\n",
        "# Main function for testing\n",
        "def test_model(model_path, image_path):\n",
        "    # Load the CLIP model\n",
        "    model = load_clip_model(model_path)\n",
        "\n",
        "    # Preprocess the input image\n",
        "    image_tensor = preprocess_image(image_path)\n",
        "\n",
        "    # Perform anomaly detection\n",
        "    detect_anomaly(model, image_tensor)\n",
        "\n",
        "# Example usage\n",
        "model_path = '/content/drive/MyDrive/SRE/eoe-main/saved_model/model_weights_cls3_seed0_epoch80.pth'\n",
        "image_path = '/content/drive/MyDrive/SRE/test_images/img_4.jpeg'\n",
        "test_model(model_path, image_path)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "5N5OK-nbfjKO",
        "outputId": "04019860-41ec-4b9e-fe46-aedab9ea5232"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e1078059df2a>\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/SRE/eoe-main/saved_model/model_weights_cls3_seed0_epoch80.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/SRE/test_images/img_4.jpeg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-e1078059df2a>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(model_path, image_path)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Perform anomaly detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mdetect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-e1078059df2a>\u001b[0m in \u001b[0;36mdetect_anomaly\u001b[0;34m(model, image_tensor)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# For example, you can print the model's prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model Prediction:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'collections.OrderedDict' object is not callable"
          ]
        }
      ]
    }
  ]
}